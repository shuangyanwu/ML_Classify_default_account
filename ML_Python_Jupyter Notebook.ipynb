{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095beca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "625f4bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb2e1200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('/Users/wushuangyan/Desktop/Linear models/base'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "789af8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/wushuangyan/Desktop/Linear models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53134fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n",
    "from base.module import GradientClassifier  # import from local files\n",
    "\n",
    "class LogisticRegression(GradientClassifier):\n",
    "    def __init__(self, feat_dim=4):\n",
    "        GradientClassifier.__init__(self, \"LogReg\")\n",
    "        self._features = feat_dim\n",
    "        self.weights = tc.nn.Parameter(tc.randn((feat_dim, 1), dtype=tc.float32))\n",
    "        self.bias = tc.nn.Parameter(tc.zeros((1, ), dtype=tc.float32) + 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 2 and x.shape[1] == self._features\n",
    "        # Implement a linear model for the log odds.\n",
    "        logits = tc.mm(x, self.weights) + self.bias \n",
    "        return tc.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9a3dee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer Perceptron Neural Networks\n",
    "from base.module import GradientClassifier\n",
    "\n",
    "class MLPBinaryClassifier(GradientClassifier):\n",
    "    def __init__(self, in_dim=4, hide_dim=64, device=\"cpu\"):\n",
    "        GradientClassifier.__init__(self, \"MLP\")\n",
    "        self._features = in_dim\n",
    "        self.weights_hide = tc.nn.Parameter(tc.randn((in_dim, hide_dim)))\n",
    "        self.bias_hide = tc.nn.Parameter(tc.zeros((hide_dim, ), dtype=tc.float32) + 0.1)\n",
    "        self.weights_clf = tc.nn.Parameter(tc.randn((hide_dim, 1)))\n",
    "        self.bias_clf = tc.nn.Parameter(tc.zeros((1, ), dtype=tc.float32) + 0.1)\n",
    "\n",
    "    def forward(self, x): \n",
    "        assert len(x.shape) == 2 and x.shape[1] == self._features\n",
    "        # Implement the MLP model.\n",
    "        logits = tc.mm((tc.mm (x, self.weights_hide) + self.bias_hide), self.weights_clf)+ self.bias_clf\n",
    "        return tc.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef318996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes model\n",
    "from base.module import StatisticClassifier\n",
    "\n",
    "class BayesianClassifier(StatisticClassifier):\n",
    "    \n",
    "    def __init__(self):\n",
    "        StatisticClassifier.__init__(self, \"NaiveBayesian\")\n",
    "        self._probas = None\n",
    "\n",
    "    def forward(self, sample):\n",
    "        target_y, target_prob = 0, 0.0\n",
    "        for y, (postior, cond_probs) in enumerate(self._probas):\n",
    "            # P(Y=y|x1, x2, ...) ~= P(Y=y) * P(X1=x1|Y=y) * P(X2=x2|Y=y) * ...\n",
    "            for x_idx, x_cond_prob_dict in enumerate(cond_probs):\n",
    "                postior *= x_cond_prob_dict.get(sample[x_idx], 0.0)\n",
    "            if  postior > target_prob:       \n",
    "                # Find the biggest postior probability\n",
    "                target_y, target_prob = y, postior           \n",
    "        return target_y            \n",
    "\n",
    "    def _fit(self, X, Y):\n",
    "        # the dimsension of is X = (num_samples, num_featues)\n",
    "        self._probas = [] # store P(y) and P(X|Y=y) as a tuple\n",
    "        for y in range(self._num_cls): # y= 0,1,2,3...\n",
    "            # calculate P(X|Y=y) by storing them as a sequence\n",
    "            subX = X[Y == y] # records with label y\n",
    "            y_cond_prob = [] # store [P(X1|Y=y), P(X2|Y=y), ...] as list\n",
    "            for subx_seq in subX.T:\n",
    "                counts = {}  # dict\n",
    "                for val in subx_seq.tolist():\n",
    "                    if val not in counts:\n",
    "                        counts[val] = 0\n",
    "                    counts[val] += 1\n",
    "                    # Calcluate the P(X1=x11|Y=y), P(X1=x21|Y=y)\n",
    "                x_probs = {key: value / len(subX) for key, value in counts.items()}\n",
    "                y_cond_prob.append(x_probs) \n",
    "                \n",
    "            Py = len(subX) / len(X) # calculate P(Y=y)\n",
    "            self._probas.append((Py, y_cond_prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7e519ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree \n",
    "from base.module import StatisticClassifier\n",
    "\n",
    "class _Tree:\n",
    "    \n",
    "    \"\"\"basic node structure for construction tree\"\"\"\n",
    "    \n",
    "    def __init__(self, label, feature=None):\n",
    "        assert (isinstance(feature, int) and 0 <= feature) or feature is None\n",
    "        assert isinstance(label, (int, np.int32, np.int64)) and 0 <= label\n",
    "        self._feature = feature\n",
    "        self._label = label\n",
    "        self._children = {} #dict\n",
    "\n",
    "    @property\n",
    "    def label(self):\n",
    "        return self._label\n",
    "    \n",
    "    @property\n",
    "    def feature(self):\n",
    "        return self._feature\n",
    "        \n",
    "    def __getitem__(self, condition):\n",
    "        return self._children.get(condition, self._label)\n",
    "\n",
    "    def __setitem__(self, condition, children):\n",
    "        assert not self.is_leaf(), 'current node is a leaf!'\n",
    "        assert isinstance(children, _Tree)\n",
    "        self._children[condition] = children\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self._feature is None\n",
    "\n",
    "\n",
    "def standard_entropy(seq):\n",
    "    assert len(seq.shape) == 1\n",
    "    uniqs, counts = np.unique(seq, return_counts=True)\n",
    "    probs = counts / seq.size\n",
    "    # entropy = -sum[P(y) * log(P(y))]\n",
    "    entropy = - sum (probs * np.log2(probs))     \n",
    "    return entropy\n",
    "\n",
    "def conditional_entropy(x_seq, y_seq):\n",
    "    assert len(x_seq.shape) == len(y_seq.shape) == 1\n",
    "    assert x_seq.size == y_seq.size\n",
    "    entropy = 0.0\n",
    "    x_uniqs, x_counts = np.unique(x_seq, return_counts=True)\n",
    "    for x_label, x_count in zip(x_uniqs, x_counts):\n",
    "        x_prob = x_count / x_seq.size\n",
    "        y_controled_by_x = y_seq[x_seq == x_label] \n",
    "        # conditional entropy = - Px * sum[P(y|x) * logP(y|x)]\n",
    "        entropy += x_prob * (sum (((np.unique(y_controled_by_x, return_counts=True)[1])/len(y_controled_by_x))*\n",
    "                                 (np.log2((np.unique(y_controled_by_x, return_counts=True)[1])/len(y_controled_by_x)))))          \n",
    "    return - entropy\n",
    "        \n",
    "\n",
    "class DecisionTreeClassifier(StatisticClassifier):\n",
    "    def __init__(self):\n",
    "        StatisticClassifier.__init__(self, \"DecisionTree\")\n",
    "        self._tree = None\n",
    "\n",
    "    def forward(self, sample):\n",
    "        tree = self._tree\n",
    "        while not tree.is_leaf():\n",
    "            tree = tree[sample[tree.feature]]\n",
    "        return tree.label\n",
    "\n",
    "    def _construct_tree(self, X, Y, used_fids):\n",
    "        # Decision tree are generated recursively. Specifically, there\n",
    "        # are three steps as following:\n",
    "        # Step-1: check whether we stop the recursion.\n",
    "        # Step-2: calculate information gain of each variable to pick up\n",
    "        #        the best variable to split data.\n",
    "        # Step-3: recursively call this function to generate children\n",
    "        #        by using the best splitting variable.\n",
    "\n",
    "        # Step-1: check whether we need to generate children\n",
    "        most_freq_y = np.bincount(Y.astype(np.int32)).argmax() # y = 0,1,2,..\n",
    "        entire_entropy = standard_entropy(Y)\n",
    "        stop_cond_1 = (len (np.unique(Y))==1)\n",
    "                     # Assign a bool value to identify: whether there is only a single y in Y.\n",
    "        stop_cond_2 = (np.unique(X, axis=0).shape[0] == 1)\n",
    "                     # Assign a bool value to identify: whether we use out all features.\n",
    "        if stop_cond_1 or stop_cond_2:\n",
    "            return _Tree(most_freq_y) \n",
    "        \n",
    "        # Step-2: find out the best splitting feature\n",
    "        max_info_gain, best_fid, best_seq = -float(\"inf\"), 0, None\n",
    "        for fid, x_seq in enumerate(X.T):\n",
    "            if fid in used_fids:\n",
    "                continue\n",
    "            x_info_gain = entire_entropy - conditional_entropy(x_seq, Y)\n",
    "            if x_info_gain > max_info_gain:\n",
    "                max_info_gain, best_fid, best_seq = x_info_gain, fid, x_seq\n",
    "\n",
    "        # Step-3: recursively generate children of the current tree\n",
    "        root = _Tree(most_freq_y, best_fid)\n",
    "        used_fids = used_fids | {best_fid}\n",
    "        for uniq_val in np.unique(best_seq):\n",
    "            uniq_idx = best_seq == uniq_val\n",
    "            subX, subY = X[uniq_idx], Y[uniq_idx]\n",
    "            # Recursively call `self._construct_tree` to generate children.\"\n",
    "            root[uniq_val] = self._construct_tree(subX, subY, used_fids)\n",
    "        return root\n",
    "\n",
    "    def _fit(self, X, Y):\n",
    "        self._tree = self._construct_tree(X, Y, set())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5499001f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: LogReg\n",
      "--------------------------------------------------------------------------------\n",
      "Train Accuracy=0.9714 | F1=0.9737 | AUC=0.9967\n",
      "Test Accuracy=0.9667 | F1=0.9600 | AUC=0.9955\n",
      "\n",
      "Model: MLP\n",
      "--------------------------------------------------------------------------------\n",
      "Train Accuracy=0.9857 | F1=0.9867 | AUC=1.0000\n",
      "Test Accuracy=0.9667 | F1=0.9600 | AUC=0.9910\n",
      "\n",
      "Model: NaiveBayesian\n",
      "--------------------------------------------------------------------------------\n",
      "Train Accuracy=0.9286 | F1=0.9296 | AUC=0.9308\n",
      "Test Accuracy=0.9333 | F1=0.9231 | AUC=0.9321\n",
      "\n",
      "Model: DecisionTree\n",
      "--------------------------------------------------------------------------------\n",
      "Train Accuracy=0.9857 | F1=0.9867 | AUC=0.9848\n",
      "Test Accuracy=0.9000 | F1=0.8800 | AUC=0.8937\n"
     ]
    }
   ],
   "source": [
    "# Build pipeline for all 4 ML methods\n",
    "from base.utils import prepare_dataset, scoring\n",
    "\n",
    "def pipeline(model, train, test):\n",
    "    name = model.name\n",
    "    trainX, trainY = train[:, :-1], train[:, -1]\n",
    "    print(\"\")\n",
    "    print(\"Model: %s\\n\" % name + \"-\" * 80)\n",
    "    model.fit(trainX, trainY)\n",
    "    \n",
    "    acc, f1, auc = scoring(trainY, model.predict(trainX))\n",
    "    print(\"Train Accuracy=%.4f | F1=%.4f | AUC=%.4f\" % (acc, f1, auc))\n",
    "\n",
    "    acc, f1, auc = scoring(test[:, -1].astype(np.int32), model.predict(test[:, :-1]))\n",
    "    print(\"Test Accuracy=%.4f | F1=%.4f | AUC=%.4f\" % (acc, f1, auc))\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    train, test, labels = prepare_dataset(\"./iris.csv\", do_normalize=True)\n",
    "    for architect in [LogisticRegression, MLPBinaryClassifier]:\n",
    "        pipeline(architect(), train, test)\n",
    "    train, test, labels = prepare_dataset(\"./iris.csv\", do_discretize=True)\n",
    "    for architect in [BayesianClassifier, DecisionTreeClassifier]:\n",
    "        pipeline(architect(), train, test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfab5d1",
   "metadata": {},
   "source": [
    "The model accuracy, F1 score, and AUC order for the test set is: Logistic Regression >= MLP > Naive Bayes > Decision Tree. Although the MLP nerual network model performs well on the training and test sets, it took much longer time to train the model. And the decision tree model seems to overfit the training data, since the model performance (accuracy, F1, and AUC) shows a large difference between the training and test sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4543bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
